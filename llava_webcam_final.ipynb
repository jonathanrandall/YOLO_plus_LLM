{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import ollama\n",
    "\n",
    "from PIL import Image\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_b_array(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        img1 = Image.open(image_file)\n",
    "        \n",
    "        # Convert the image to a byte stream in JPEG format\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        img1.save(img_byte_arr, format='JPEG')  # or 'PNG' if you prefer\n",
    "        img_byte_arr = img_byte_arr.getvalue()\n",
    "    return img_byte_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_webcam_index(device_name):\n",
    "    command = \"v4l2-ctl --list-devices\"\n",
    "    output = subprocess.check_output(command, shell=True, text=True)\n",
    "    devices = output.split('\\n\\n')\n",
    "    \n",
    "    for device in devices:\n",
    "        if device_name in device:\n",
    "            lines = device.split('\\n')\n",
    "            for line in lines:\n",
    "                if \"video\" in line:\n",
    "                    parts = line.split()\n",
    "                    for part in parts:\n",
    "                        if part.startswith('/dev/video'):\n",
    "                            print(part)\n",
    "                            return (part[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam_index = int(find_webcam_index(\"C922\")) #C922 #3D USB\n",
    "webcam_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the resolution\n",
    "width = 640 #320  # Desired width\n",
    "height = 480 # 240  # Desired height\n",
    "mult = 1\n",
    "cap = cv2.VideoCapture(webcam_index)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video stream.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # If the frame was read correctly\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to read frame.\")\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Camera Stream', frame)\n",
    "\n",
    "    # Press 'q' to quit the stream\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.imwrite('frame.jpg', frame)\n",
    "        break\n",
    "\n",
    "\n",
    "# Release the capture and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "img1 = cv2.imread('frame.jpg')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_img = get_b_array('frame.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model='llava', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Extract the objects in the image. Return only a plain list of object names, comma-separated, no other words.',\n",
    "        #'List only the objects you see in this image. Respond with a simple, comma-separated list with no explanation.',\n",
    "        'images': [bus_img],\n",
    "    },\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo pkill ollama\n",
    "text = (response['message'])['content']\n",
    "print(\"Response:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // I need to write file to predict with yolo world.\n",
    "model = YOLO(\"yolov8m-world.pt\")  # load a pretrained model (recommended for best performance)\n",
    "\n",
    "s = text\n",
    "objects = [item.strip() for item in s.split(',')]\n",
    "model.set_classes(objects) #[\"vheicle\", \"person\", \"road\"])  # set custom class names\n",
    "\n",
    "results=model.predict(source=\"frame.jpg\", save=True, conf=0.005)  # predict on an image with confidence threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = results[0]\n",
    "\n",
    "# Get the annotated image (with bounding boxes)\n",
    "annotated_img = result.plot()\n",
    "\n",
    "# OpenCV returns BGR format, convert to RGB for matplotlib\n",
    "import cv2\n",
    "annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(annotated_img_rgb)\n",
    "plt.axis('off')\n",
    "plt.title(\"YOLO Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"frame.jpg\")\n",
    "detections = sv.Detections.from_ultralytics(result).with_nms(threshold=0.0, class_agnostic=False)\n",
    "detections = detections[detections.confidence > 0.01]\n",
    "\n",
    "# Annotate filtered detections\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "labels = [\n",
    "    f\"{result.names[class_id]} {confidence:.2f}\"\n",
    "    for class_id, confidence in zip(detections.class_id, detections.confidence)\n",
    "]\n",
    "\n",
    "annotated_img = box_annotator.annotate(\n",
    "    scene=image.copy(), detections=detections)\n",
    "annotated_img = label_annotator.annotate(\n",
    "    scene=annotated_img, detections=detections, labels=labels)\n",
    "\n",
    "# annotated_img = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
    "\n",
    "# Convert BGR to RGB for matplotlib\n",
    "annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(annotated_img_rgb)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"YOLO with NMS-filtered Detections\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# response = ollama.chat(model='llava', messages=[\n",
    "#     {\n",
    "#         'role': 'user',\n",
    "#         'content': 'Extract the objects in the image. Return only a plain list of object names, comma-separated, no other words.',\n",
    "#         'images': [bus_img],\n",
    "#     },\n",
    "# ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_yolo_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
